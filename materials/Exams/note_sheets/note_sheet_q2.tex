\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{hyperref}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {2ex} %{-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries\underline}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {1ex} %{-1ex plus -.5ex minus -.2ex}%
                                {0.8ex} %{0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{2}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
   \Large{\textbf{STAT 305 Quiz II}} \\
   \Large{\textbf{ Reference Sheet  }}\\
\end{center}

\section{Numeric Summaries}
\begin{tabular}{@{}ll@{}}
        & \\
mean    & $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ \\
        & \\
population variance  & $\sigma^2 = \frac{1}{n}\sum_{i=1}^n \left(x_i - \bar{x} \right)^2$ \\
        & \\
population standard deviation  & $\sigma = \sqrt{\frac{1}{n}\sum_{i=1}^n \left(x_i - \bar{x} \right)^2}$ \\
        & \\
sample variance  & $s^2 = \frac{1}{n-1}\sum_{i=1}^n \left(x_i - \bar{x} \right)^2$ \\
        & \\
sample standard deviation  & $s = \sqrt{\frac{1}{n-1}\sum_{i=1}^n \left(x_i - \bar{x} \right)^2}$ \\
        & \\
\end{tabular}

\vspace{.2cm}

\section{Linear Relationships}
\begin{tabular}{@{}ll@{}}
        & \\
Form & $y \approx \beta_0 + \beta_1 x$ \\
        & \\
Fitted linear relationship & $\hat{y} = b_0 + b_1 x$ \\
        & \\
Least squares estimates & $b_1 = \frac{\sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$ \\
        & \\
                        & $b_1 = \frac{ \sum_{i = 1}^n x_i y_i - n \bar{x} \bar{y} }{ \sum_{i = 1}^n x_i^2 - n \bar{x}^2 } $ \\
        & \\
                        & $b_0 = \bar{y} - b_1 \bar{x}$ \\
        & \\
Residuals & $e_i = y_i - \hat{y}_i $ \\
        & \\
sample correlation coeffecient & $r = \frac{\sum_{i=1}^n \left(x_i - \bar{x} \right)\left(y_i - \bar{y}\right)}{\sqrt{\sum_{i=1}^n \left( x_i - \bar{x} \right)^2 \sum_{i=1}^n \left(y_i - \bar{y} \right)^2}}$ \\
        & \\
                               & $r = \frac{\sum_{i=1}^n x_i y_i - n \bar{x} \bar{y}}{\sqrt{\left(\sum_{i=1}^n x_i^2 - n \bar{x}^2\right)\left(\sum_{i=1}^n y_i^2 - n\bar{y}^2\right)}}$ \\
        & \\
coeffecient of determination & $R^2 = (r)^2$ \\
        & \\
                             & $\frac{\sum_{i=1}^n \left(y_i - \bar{y} \right)^2 - \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2}{\sum_{i=1}^n \left(y_i - \bar{y}\right)^2}$ \\
        & \\
\end{tabular}

\vspace{.2cm}

\section{Multivariate Relationships}
\begin{tabular}{@{}ll@{}}
        & \\
Form & $y \approx \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k $ \\
        & \\
Fitted relationship  & $\hat{y} \approx b_0 + b_1 x_1 + b_2 x_2 + \ldots + b_k x_k $ \\
        & \\
Residuals & $e_i = y_i - \hat{y}_i $ \\
        & \\
Sums of Squares & $\text{SSTO} = \sum_{i = 1}^n \left(y_i - \bar{y}\right)^2$ \\
        & \\
                & $\text{SSE} = \sum_{i = 1}^n \left(y_i - \hat{y}_i\right)^2$ \\
        & \\
                & $\text{SSR} = SSTO - SSE = \sum_{i = 1}^n \left(\hat{y}_i - \bar{y}\right)^2$ \\
        & \\
coeffecient of determination & $R^2 = \frac{ \sum_{i = 1}^n \left(y_i - \bar{y}\right)^2 - \sum_{i = 1}^n \left(y_i - \hat{y}_i\right)^2 }{ \sum_{i = 1}^n \left(y_i - \bar{y}\right)^2 }$ \\
        & \\
                             & $R^2 = \frac{ \text{SSTO} - \text{SSE} }{\text{SSTO}}$ \\
        & \\
                             & $R^2 = \frac{ \text{SSR} }{\text{SSTO}}$ \\
        & \\
                             & $\frac{\sum_{i=1}^n \left(y_i - \bar{y} \right)^2 - \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2}{\sum_{i=1}^n \left(y_i - \bar{y}\right)^2}$ \\
        & \\
\end{tabular}

\vspace{.2cm}

\section{Functions}

\textbf{Quantile Function $Q(p)$} For a univariate sample consisting of
\(n\) values that are ordered so that \(x_1 \le x_2 \le \ldots \le x_n\)
and value \(p\) where \(0 \le p \le 1\), let
\(i = \lfloor n \cdot p + 0.5 \rfloor\). Then the quantile function at
\(p\) is:

\[
Q(p) = x_i + (n \cdot p + 0.5 - i)(x_{i+1} - x_i)
\]

\newpage

\section{Basic Probability}

\subsection{Definitions}
\begin{tabular}{@{}ll@{}}
   Random experiment & A series of actions that lead to an observable result. \\
        & The result may change each time we perform the experiment. \\
        & \\
   Outcome & The result(s) of a random experiment. \\
        & \\
   Sample Space ($S$) & A set of all possible results of a random experiment. \\
        & \\
   Event ($A$) & Any subset of sample space. \\
        & \\
   Probability of an event ($P(A)$) & the likelihood that the observed outcome of \\
    & a random experiment is one of the outcomes in the event. \\
        & \\
   $A^C$ & The outcomes that are not in $A$. \\
   $A \cap B$ & The outcomes that are both in $A$ and in $B$. \\
   $A \cup B$ & The outcomes that are either $A$ or $B$. \\
\end{tabular}

\subsection{General Rules}

\begin{tabular}{@{}ll@{}}
   Probability $A$ given $B$ & $P(A | B) = P(A \cap B)/P(B)$ \\
        & \\
   Probability $A$ and $B$ & $P(A \cap B) = P(A | B) P(B) = P(B | A) P(A)$ \\
        & \\
   Probability $A$ or $B$ & $P(A \text{ or } B) = P(A) + P(B) - P(A, B)$ \\
        & \\
\end{tabular}

\subsection{Independence}

Two events are called independent if \(P(A, B) = P(A) \cdot P(B)\).
Clever students will realize this also means that if \(A\) and \(B\) are
independent then \(P(A|B) = P(A)\) and \(P(B|A) = P(B)\).

\subsection{Joint Probability}

\begin{tabular}{@{}ll@{}}
        & \\
   Joint Probability & The probability an outcome is in event $A$ and in event $B$ = $P(A, B)$. \\
        & \\
   Marginal Probability & If $A \subseteq B \cup C$ then $P(A) = P(A \cap B) + P(A \cap C)$. \\
        & \\
   Conditional Probability & For events $A$ and $B$, if $P(B) \ne 0$ then $P(A|B) = P(A \cap B)/P(B)$. \\
        & \\
\end{tabular}

\section{Discrete Random Variables}

\subsection{General Rules}

\begin{tabular}{@{}ll@{}}
        & \\
   Probability function &  $f_X(x) = P(X = x)$ \\
        & \\
   Cumulative probability function &  $F_X(x) = P(X \le x)$ \\
        & \\
   Expected Value & $\mu = E(X) = \sum_{x} x f_X(x)$ \\
        & \\
   Variance & $\sigma^2 = Var(X) = \sum_{x} (x - \mu)^2 f_X(x)$ \\
        &\\
            & or, $Var(X) = \sum_{i=1}^n x_i^2 \cdot f(x_i) -  \mu^2$\\
        &\\
            & or, $Var(X) =\sum\limits_x(x - \text{E}X)^2f(x) = \text{E}(X^2) - (\text{E}X)^2$\\
        & \\
        
   Standard Deviation & $\sigma = \sqrt{Var(X)}$ \\
        & \\
\end{tabular}

\subsection{Joint Probability Functions}

\begin{tabular}{@{}ll@{}}
        & \\
   Joint Probability Function & $ f_{XY}(x,y) = P[X = x, Y = y] $ \\
        & \\
   Marginal Probability Function & $f_X(x) = \sum_{y} f_{XY}(x,y)$ \\
                                 & $f_Y(y) = \sum_{x} f_{XY}(x,y)$ \\
        & \\
   Conditional Probability Function & $f_{X|Y}(x|y) = f_{XY}(x,y)/f_{Y}(y)$ \\
                                    & $f_{Y|X}(y|x) = f_{XY}(x,y)/f_{X}(x)$ \\
        & \\
\end{tabular}

\subsection{Geometric Random Variables}

\(X\) is the trial count upon which the first successful outcome is
observed performing independent trials with probability of success
\(p\).

\begin{tabular}{@{}ll@{}}
        & \\
   Possible Values & $x = 1, 2, 3, \ldots$ \\
        & \\
   Probability function &  $P[X = x] = f_X(x) = p (1-p)^{x-1}$\ \\
        & \\
   Expected Value & $\mu = E(X) = \frac{1}{p} $ \\
        & \\
   Variance & $\sigma^2 = Var(X) = \frac{1 - p}{p^2}$ \\
        & \\
\end{tabular}

\subsection{Binomial Random Variables}

\(X\) is the number of successful outcomes observed in \(n\) independent
trials with probability of success \(p\).

\begin{tabular}{@{}ll@{}}
        & \\
   Possible Values & $x = 0, 1, 2, \ldots, n$ \\
        & \\
   Probability function &  $P[X = x] = f_X(x) = \frac{n!}{(n-x)!x!} p^x (1-p)^{n-x}$ \\
        & \\
   Expected Value & $\mu = E(X) = n p $ \\
        & \\
   Variance & $\sigma^2 = Var(X) = n p (1-p)$ \\
        & \\
\end{tabular}

\subsection{Poisson Random Variables}

\(X\) is the number of times a rare event occurs over a predetermined
interval (an area, an amount of time, etc.) where the number of events
we expect is \(\lambda\).

\begin{tabular}{@{}ll@{}}
        & \\
   Possible Values & $x = 0, 1, 2, 3, \ldots$ \\
        & \\
   Probability function &  $P[X = x] = f_X(x) = \frac{e^{-\lambda} \lambda^x}{x!}$ \\
        & \\
   Expected Value & $E(X) = \lambda $ \\
        & \\
   Variance & $Var(X) = \lambda$ \\
        & \\
\end{tabular}

\end{multicols}
\end{document}

